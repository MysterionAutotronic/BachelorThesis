\documentclass[11pt, a4paper, oneside, listof=totoc]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman, english]{babel}
\usepackage[onehalfspacing]{setspace}
\usepackage{helvet}
\usepackage[a4paper, left=2.5cm, right=2.5cm]{geometry}
\usepackage[
    backend=biber,
    style=authoryear,
    citestyle=authoryear,
    sorting=nyt,
    sortcites=true,
    dashed=false,
    giveninits=false,
    uniquename=true,
    maxbibnames=999,
    url=true
]{biblatex}
\usepackage{csquotes}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{courier}
\usepackage{needspace}
\usepackage{trace}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage[automark]{scrlayer-scrpage}
\usepackage{microtype}
\usepackage{svg}
\usepackage{xurl}
\usepackage{enumitem}
\usepackage{csquotes}
\usepackage[acronym]{glossaries}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{mfirstuc}
\usepackage{array}
\usepackage{setspace}

% show frames in draft
%\usepackage{showframe}

% set language to english
\selectlanguage{english}

% set font to sans-serif
\renewcommand{\familydefault}{\sfdefault}

\addbibresource{jabref-library.bib}

% custom vars
\newcommand{\thesistitle}{Conception, Implementation, and Evaluation of a Highly Scalable and Highly Available Kubernetes-Based SaaS Platform on Kubernetes Control Plane (KCP)}
\newcommand{\sic}{\textnormal{\textit{[sic]}}}
\newcommand{\see}[1]{(see~\autoref{#1}: \textit{\nameref{#1}})}
\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}

% URL formatting
\PassOptionsToPackage{hyphens}{url}

% metadata
\title{\thesistitle}
\author{David Linhardt}
\date{\today}

\pagestyle{scrheadings}
\cfoot{\thepage} % Page number in center of footer

% glossary
\makeglossaries{}
\input{glossary.tex}
\input{acronyms.tex}

\begin{document}

    \newgeometry{top=1cm, bottom=1cm, left=2.5cm, right=2.5cm}

    \begin{titlepage}
        \thispagestyle{empty}

        \hspace*{-1.5cm}
        \noindent
        \hfill
        % THI Logo
        \begin{minipage}{0.3\textwidth}
            \raggedleft\
            \hspace*{1cm}
            \includegraphics[width=1\textwidth]{images/thi_logo.pdf}
        \end{minipage}

        \vspace{1cm}

        \hrulefill{}

        \vspace{4cm}

        % header box
        \noindent
        \makebox[\textwidth][c]{
            \parbox{10cm}{
                \LARGE\textbf{Technische Hochschule Ingolstadt}\\
                \\
                \Large\textbf{Specialist area Computer Science}\\
                \Large\textbf{Bachelor's course Computer Science}\\
            }
        }

        % bachelors thesis
        \begin{center}
            \LARGE\textbf{Bachelor's thesis}
        \end{center}

        \vspace{1cm}

        % tabularx
        \begin{tabularx}{\textwidth}{@{}lX@{}}
            \textbf{Subject:} & \thesistitle\\[2cm]
            \textbf{Name and Surname:} & David Linhardt \\[0.5cm]
            \textbf{Matriculation number:} & 00122706\\[2cm]     
            \textbf{Issued on:} & 2025--04--09 \\[0.5cm]           
            \textbf{Submitted on:} & 2025--08--01 \\[2cm]           
            \textbf{First examiner:} & Prof.\ Dr.\ Bernd Hafenrichter \\[0.5cm]      
            \textbf{Second examiner:} & Prof.\ Dr.\ Ludwig Lausser \\
        \end{tabularx}

    \end{titlepage}

    \begin{titlepage}
        \hspace*{-1.5cm}
        \noindent
        \hfill
        % THI Logo
        \begin{minipage}{0.3\textwidth}
            \raggedleft\
            \hspace*{1cm}
            \includegraphics[width=1\textwidth]{images/thi_logo.pdf}
        \end{minipage}

        \vspace{1cm}

        \LARGE\textbf{Declaration in Accordance with §~30~Abs.~4~Nr.~7~APO~THI}\\

        \vspace{1cm}

        \hrulefill{}

        \vspace{2cm}
        
        \begin{center}
            \LARGE\textbf{Declaration}\\
        \end{center}
        \vspace{1cm}
        \normalsize
        I hereby declare that this thesis is my own work, that I have not presented it elsewhere for
        examination purposes and that I have not used any sources or aids other than those stated.
        I have marked verbatim and indirect quotations as such.\\[1cm]
        Ingolstadt, 2025--08--01\\[2cm]
        David Linhardt

    \end{titlepage}

    \restoregeometry{}

    \pagenumbering{roman}

    \section*{Abstract}\label{abstract}
    \markboth{Abstract}{Abstract}

    \newpage

    % Glossary
    \printglossary[type=\acronymtype, title={Acronyms}]
    \cleardoublepage{}
    \printglossary[title={Glossary}]


    \cleardoublepage{}

    % TOC
    \begingroup
        \microtypesetup{protrusion=false}
        \tableofcontents
    \endgroup

    \newpage

    \cleardoublepage{}
    \begingroup
        \renewcommand{\addcontentsline}[3]{}
        \listoffigures
    \endgroup

    \cleardoublepage{}
    \begingroup
        \renewcommand{\addcontentsline}[3]{}
        \listoftables{}
    \endgroup

    \newpage

    \pagenumbering{arabic}

    \section{Introduction}\label{sec:introduction}

        \subsection{Problem Statement and Motivation}\label{subsec:problem}

        \subsection{Objectives and Scope}\label{subsec:objectives}

        \subsection{Structure of the Thesis}\label{subsec:structure}

    \section{Fundamentals}

        \subsection{Kubernetes and Multi-Tenancy}\label{subsec:k8sAndMultiTenancy}

            \subsubsection{Kubernetes as the Foundation for Cloud-Native Applications}\label{subsubsec:foundationK8s}
                As the de facto standard for deploying and managing 
                \textit{cloud-native applications}, \gls{k8s@gls}, commonly referred to as \gls{k8s}
                plays a pivotal role in modern cloud architecture \parencite[p.~7--8]{poulton2021}.
                \gls{k8s@gls} works as an  orchestrator for \textit{containerized,
                cloud-native microservice} applications, meaning it can deploy apps and dynamically
                respond to changes \parencite[p.~3]{poulton2021}.
                It offers a platform for declarative configuration and automation for containerized
                workloads, enabling organizations to run distributed applications and services at
                scale \parencite{kubernetesOverview,redhatWhatIsKubernetes}.

            \subsubsection{The Importance of Multi-Tenancy in Modern SaaS Platforms}\label{subsubsec:mtImportance}
                Multi-tenancy plays a fundamental role in modern cloud computing.
                By allowing multiple tenants to share the same infrastructure through
                virtualization, it significantly increases resource utilization, reduces operational
                costs, and enables essential features such as VM mobility and dynamic resource
                allocation \parencite[pp.~345--346]{aljahdali2014}. 
                These benefits are crucial for cloud providers, as they make the cloud business
                model economically viable and scalable.
                In the context of modern \gls{saas} platforms, \gls{multiTenancy} goes even further
                by enabling unified management, frictionless onboarding, and simplified operational
                processes that allow providers to add new tenants without introducing incremental
                complexity or cost \parencite[pp.~9--11]{awsSaaSArchitectureFundamentals}.
                \\
                However, while \gls{multiTenancy} is indispensable for achieving efficiency,
                scalability, and cost-effectiveness, it simultaneously introduces complex security
                challenges, especially in shared environments where resource isolation is limited. 
                In particular, the potential for cross-tenant access and side-channel attacks makes
                security in multi-tenant environments a primary concern
                \parencite[pp.~345--346]{aljahdali2014}. 
                As such, understanding and addressing \gls{multiTenancy} from both operational and
                security perspectives is essential when designing and securing modern cloud-native
                platforms
                \parencites[pp.~9--11]{awsSaaSArchitectureFundamentals}[p.~4]{isoConcepts}.

            \subsubsection{The Challenges of Multi-Tenancy and the Need for Solutions}\label{subsubsec:challenges}
                Multi-tenancy introduces a spectrum of technical and security challenges that need
                to be addressed.

                \begin{enumerate}[label={[\arabic*]:},
                    ref=Challenge~\arabic*,
                    leftmargin=*,
                    itemsep=0.6\baselineskip]

                    \item\label{chal:remanence}
                        \textit{Residual-data exposure}.
                        Shared infrastructures may expose tenants to data leakage and hardware-layer
                        attacks. 
                        Because hardware resources are only virtually partitioned,
                        residual data left in reusable memory or storage blocks,
                        known as \textit{data remanence},
                        can be inadvertently leaked or deliberately harvested by co-resident tenants
                        \parencites[p.~586]{zissis2012}[pp.~344--345]{aljahdali2014}.

                    \item\label{chal:transparency}
                        \textit{Control and transparency}.
                        By design, \gls{saas} moves both data storage and security controls out of
                        the enterprise's boundary and into the provider's multi-tenant cloud,
                        depriving organizations of direct oversight and assurance and thereby
                        heightening concern over how their critical information is protected,
                        replicated and kept available \parencite[pp.~3--4]{subashini2011}.
                        To complicate matters further, the customer might have no way to evaluate
                        the \gls{saas} vendors security measures, meaning the pricing and feature
                        set will most likely determine which service is used in practice, often
                        disregarding security concerns
                        \parencites[p.~6]{everett2009}[p.~836]{khorshed2012}.

                    \item\label{chal:scheduling}
                        \textit{Scheduling}.
                        In multi-tenant architectures multiple tenants utilize the same hardware,
                        thus creating the need for fair scheduling to ensure cost-effectiveness
                        and performance \parencite[p.~32597]{simi2024}.
                        Achieving fair and efficient resource allocation in scheduling first
                        requires a quantitative assessment of the system's existing unfairness
                        \parencites[p.~7]{ebrahimi2012}[p.~14]{beltre2019}[pp.~2--3]{ghodsi2011}.
                        Various scheduling algorithms and policies can be employed in practice to
                        achieve fairness \parencites[pp.~14--16]{beltre2019}[p.~4]{ghodsi2011}.
                        To fully leverage the advantages of multi-tenant architectures, resources
                        must not only be shared fairly, but also efficiently, not hindering
                        performance \parencite[p.~14]{beltre2019}.
                        As stated by~\cite[p.~14]{beltre2019} \enquote{Balancing both cluster
                        utilization and fairness is challenging}.
                    
                    \item\label{chal:isolation}
                        \textit{Performance Isolation}.
                        A single tenant is able to significantly degrade the performance of other
                        tenants working on the same hardware, if \textit{\gls{performanceIsolation}}
                        is not given \parencite[p.~195]{krebs2013}.
                        The fundamental performance expectations of a system are commonly formalized
                        in a \acrfull{sla}.% manual acrfull bcs of gls in glossary
                        As noted by~\cite[p.~195]{krebs2013} \enquote{A system is said to be
                        performance isolated, if for tenants working within their quotas the
                        performance is within the (response time) \gls{sla} while other
                        tenants exceed their quotas (e.g., request rate)}.
                        As noted by~\cite[p.~18]{carrion2022} \enquote{Currently, it is difficult
                        to achieve \gls{performanceIsolation} for \gls{multiTenancy} on \gls{k8s}
                        clusters because this requires providing resource isolation and improving
                        the abstraction of applications.}

                    \item\label{chal:automation}
                        \textit{Automation}.
                        As noted by~\cite[p.~651]{nguyen2022} \enquote{Presently, multi-tenant
                        systems lack the facility of allowing clients to dynamic~\sic{}
                        change their resources based on their business demands or create and
                        allocate resources for new tenants.
                        Multi-tenant system~\sic{} administrator manually does all the work of
                        allocation or changing tenant's~\sic{} resources.}
                        However, to ensure efficiency and scalability, an \gls{api} that allows
                        automating deployments and dynamic changes in the application is needed.

                \end{enumerate}

                A secure solution, keeping multi-tenancies advantages while also addressing security
                concerns is desperately needed
                \parencites[p.~346]{aljahdali2014}[pp.~14576--14577]{senel2023}.

            \subsubsection{Kubernetes Control Plane (KCP) as a Promising Approach}\label{subsubsec:kcpApproach}
                \acrfull{kcp} offers three capabilities that map accurately onto today's
                \gls{multiTenancy} pain points.% manual acrfull bcs glossary

                \begin{enumerate}[label={[\arabic*]:},
                    ref=Challenge~\arabic*,
                    leftmargin=*,
                    itemsep=0.6\baselineskip]

                    \item\label{chal:workspaces}
                        \textit{Workspaces}.
                        \gls{kcp} achieves strong resource isolation through the concept of
                        \textit{workspaces} \see{subsubsec:workspaces}.

                    \item\label{chal:api}
                        \textit{API}.
                        \gls{kcp} offers an \gls{k8s@gls}-like \gls{api} that enables the use of
                        standard tools and automation to a degree \see{subsubsec:api}.
                    
                    \item\label{chal:sharding}
                        \textit{Sharding}.
                        \gls{kcp} offers sharding out of the box to manage high traffic and 
                        geo-distribution \see{subsubsec:sharding}.
                \end{enumerate}

            \subsubsection{Background: The Evolution of Kubernetes}\label{subsubsec:k8sEvolution}
                \gls{k8s@gls}, an open-source \gls{container} orchestration platform developed by
                Google, emerged from the need to manage the complexities of containerized
                applications effectively and to support large-scale deployments in a cloud-native
                environment \parencites{googlecloudWhatIsKubernetes}{kubernetesOverview}.
                It was originally developed at Google and released as open source in 2014
                \parencite{googlecloudWhatIsKubernetes}.
                \gls{k8s@gls} was conceived as a successor to Google's internal \gls{container}
                management system called Borg, and designed to streamline the process of deploying,
                scaling, and managing applications composed of microservices running in containers
                \parencites[pp.~13--14]{verma2015}[p.~84]{bernstein2014}.
                Since its inception, \gls{k8s@gls} has gained traction among organizations because it
                provides robust features such as automated scaling,
                self-healing, and service discovery, which have made it the de facto standard for
                \gls{container} orchestration in the tech industry
                \parencite[pp.~855--858]{damarapati2025}.
                \\
                As noted by~\cite[p.~457]{moravcik2022} almost 90\% of organizations used
                \gls{k8s@gls} as an orchestrator for managing containers and over 70\% of organizations
                used it in production by 2021 \parencite{redhatStateOfK8sSecurityReport2021}.
                The widespread adoption of \gls{k8s@gls} is further underscored by Red Hat's latest
                (2024) report, which no longer asks survey respondents if they use \gls{k8s@gls} for
                \gls{container} orchestration, but rather \textbf{which} \gls{k8s@gls} platform they use
                \parencite[p.~27]{redhatStateOfK8sSecurityReport2024}.
                According to~\cite[pp.~855--856]{damarapati2025}, \gls{k8s@gls} has seen
                unprecedented industry adoption due to its vendor neutrality, strong community
                support, and flexible, extensible architecture in combination with readiness for
                enterprise use caused by \gls{ha@gls}, disaster recovery and security.
                \\
                Moreover \gls{k8s@gls} enables faster time-to-market by providing a unified,
                declarative control plane that abstracts away infrastructure,
                guarantees consistent environments from development to production,
                and automates operational tasks such as scaling, rolling updates,
                and self-healing—advantages that translate directly into competitive delivery speed,
                increasing its appeal to organizations of every size
                \parencite[pp.~858--859]{damarapati2025}.
                \\
                Over the years, Kubernetes --- and the many orchestration solutions inspired by or
                built on it --- has evolved to handle an increasingly diverse range of workloads,
                supporting everything from conventional applications in to emerging
                \textit{edge-native} deployments \parencites[p.~21]{biot2025}[pp.~1--4]{biot2025}.
                Edge-native deployments are applications intended to run on computing
                resources located at or near the data source --- the network \textit{edge} ---
                rather than in a central cloud \parencite[p.~34]{satyanarayanan2019}.
                This adaptability reflects its fundamental design, which focuses on modularity and
                extensibility, allowing developers to customize their orchestration needs.
                \\
                Overall, the history of \gls{k8s@gls} showcases a transformative journey driven by the
                evolving demands of software architecture and the necessity for efficient
                application management in an increasingly complex technological landscape.

            \subsubsection{Background: Containerization as an Enabler of Kubernetes}\label{subsubsec:containerization}
                \textit{\Gls{containerization}} is a way to bundle an application's code with all its
                dependencies to run on any infrastructure thus enhancing portability
                \parencite{awsWhatIsContainerization,dockerWhatContainer}.
                The lightweight nature and isolation of \glspl{container} can be leveraged by
                cloud-native software to enable both vertical and horizontal autoscaling,
                facilitated by fast startup times, as well as self-healing mechanisms and support
                for distributed, resilient infrastructures
                \parencites{kubernetesAutoscalingWorkloads}{kubernetesSelfHealing}
                    {awsWhatIsContainerization}[pp.~58--59]{davis2019}.
                Furthermore it complements the microservice architectural pattern by enabling
                isolated, low overhead deployments, ensuring consistent environments
                \parencite[p.~209]{balalaie2016}.

            \subsubsection{Background: The Role of Microservices in Cloud-Native Architectures}\label{subsubsec:microservices}
                \textit{Microservices} play a pivotal role in cloud-native architectures by
                promoting business agility, scalability, and maintainability of applications.
                By decomposing applications into independent, granular services, microservices
                facilitate development, testing, and deployment using diverse technology stacks,
                enhancing interoperability across platforms
                \parencites[p.~1]{waseem2020}[p.~1]{larrucea2018}.
                Additionally, they help prevent failures in one component from propagating across
                the system by isolating functionality into distinct, self-contained services
                \parencite[p.~62]{davis2019}.
                This architectural style aligns well with cloud environments, as it allows services
                to evolve independently, effectively addressing challenges associated with scaling
                and maintenance without being tied to a singular technological framework
                \parencite[pp.~202--203]{balalaie2016}.
                Furthermore, the integration of microservices with platforms like \gls{k8s@gls}
                enhances deployment automation and orchestration, thus providing substantial
                elasticity to accommodate fluctuating workloads \parencite[p.~170]{haugeland2021}.
                Additionally, migrating legacy applications to microservices can foster
                modernization and efficiency, thus positioning organizations favorably in
                competitive landscapes \parencite[p.~214]{balalaie2016}.
                Overall, the synergy between microservices and cloud-native architectures stems from
                their inherent capability to optimize resource utilization and streamline continuous
                integration and deployment processes.

            \subsubsection{Background: Kubernetes Resource Isolation Mechanisms}\label{subsubsec:k8sResourceIsolation}
                \gls{k8s@gls} employs several resource isolation mechanisms, primarily through the
                use of \textit{\gls{cgroups}} and \textit{namespaces} to limit
                resource allocation for containers.
                \Gls{cgroups} are a Linux kernel feature that organizes processes into hierarchical
                groups for fine-grained resource limitation and monitoring via a pseudo-filesystem
                called \textit{cgroupfs} \parencites{kubernetesCgroupsV2}{cgroups7}.
                \textit{Namespaces} are a mechanism for isolating groups of resources within a
                single cluster and scoping resource names to prevent naming conflicts across
                different teams or projects \parencite{kubernetesNamespaces}.
                However, these mechanisms may not always provide the sufficient isolation needed for
                multi-tenant architectures, because the logical segregation offered by namespaces
                does not address the fundamental security concerns associated with
                \gls{multiTenancy} \parencite[p.~651]{nguyen2022}.
                Additionally, research indicates that the native isolation strategies can lead to
                performance interference, where containers that share nodes can experience
                significant degradation in performance due to CPU contention
                \parencite[p.~158]{kim2021}.
                Specifically, critical services may be adversely affected when non-critical services
                monopolize available resources, which undermines the quality of service in
                multi-tenant environments \parencite[p.~30410]{li2019}.
                \\
                Moreover, while \gls{k8s@gls} allows for \gls{container} orchestration and resource
                scheduling, it can lead to resource fragmentation, further exacerbating the issue
                of \gls{performanceIsolation} \parencite[p.~1]{jian2023}.
                A common approach in multi-tenant scenarios is to deploy separate clusters for each
                tenant, which incurs substantial overhead—particularly in environments utilizing
                virtual machines for isolation \parencite[pp.~144574--144575]{senel2023}.
                In summary, although \gls{k8s@gls} offers essential isolation mechanisms, the
                complexities of resource sharing and performance consistency in multi-tenant
                applications highlight the need for enhanced strategies to ensure robust resource
                management and \gls{performanceIsolation}
                \parencites[p.~651]{nguyen2022}[p.~2]{jian2023}[p.~158]{kim2021}.

            \subsubsection{Relevance to SaaS and this Thesis}

        \newpage

        \subsection{Kubernetes Control Plane (KCP)}\label{subsec:kcp}
            \gls{kcp} is \enquote{An open source horizontally scalable control plane for
            \gls{k8s@gls}-like APIs} \parencite{kcpio}.
            % nguyen 2022
            % KCP docs

            \subsubsection{Workspaces}\label{subsubsec:workspaces}
                \gls{kcp} introduces the concept of \textit{workspaces} to implement
                \gls{multiTenancy}.
                In \gls{kcp}, a workspace is a \gls{k8s@gls}-cluster-like HTTPS endpoint exposed
                under \texttt{/clusters/<parent>:<name>}, that regular tools such as
                \textit{kubectl}, \textit{Helm} or \textit{client-go} treat exactly like a real
                \gls{k8s@gls} cluster.
                Every workspace is backed by its own logical cluster stored in an isolated
                \textbf{\gls{etcd} prefix}, so objects in one workspace (including cluster-scoped
                resources like \textbf{CRDs}) are completely invisible to others, delivering hard
                \gls{multiTenancy} without spinning up separate control planes
                \parencite{kcpWorkspaces}.
                \\
                As per the definition by the~\cite{etcd}, \enquote{\textbf{\gls{etcd}} is a strongly
                consistent, distributed key-value store that provides a reliable way to store data
                that needs to be accessed by a distributed system or cluster of machines.
                It gracefully handles leader elections during network partitions and can tolerate
                machine failure, even in the leader node.}
                \gls{etcd} is the primary datastore used by \gls{kcp} and \gls{k8s}
                \parencites{kcpDevStorageToRest}[p.~214]{sun2021}.
                \textbf{\gls{etcd} prefixes} are a simple, inbuilt way to group keys using a prefix
                \parencite{etcdPrefix}.
                This allows for resource isolation in \gls{kcp}.\@
                A \gls{crd} is a declaratively specified schema that registers a new  resource,
                defined by its group, version, kind, and OpenAPI schema, into the \gls{kcp@gls} so
                the native \gls{api} server stores and serves the objects as first-class
                resources \parencite{kubernetesCRD}.
                \\
                \gls{kcp} implements the same \gls{rbac}-based authorization mechanism and cluster
                role and cluster role binding principles as \gls{k8s@gls} inside workspaces 
                \parencite{kcpAuthorization}.
                However unlike \gls{k8s@gls} \gls{kcp} does currently not support \gls{abac}
                \parencites{kcpAuthorization}{kubernetesABAC}.
                \gls{kcp} likely supports only \gls{rbac}-based authorization because \gls{abac} is
                considered overly complex, hard to audit, and increasingly deprecated in favor of
                \gls{rbac}, which offers a more structured and maintainable access control model
                \parencite{kubernetesRBACBlog}.
                This allows for consistent access control and permission management across all
                workspaces, aligning with familiar \gls{k8s@gls} patterns and simplifying
                multi-tenant environment administration.
                \\
                Workspaces allow for a typed, parent-child tree, and each type can constrain which
                kind of workspaces it can contain or be contained by, giving platform teams a
                structured way to delegate environments while retaining policy control
                \parencite{kcpWorkspaces}.
                \\
                This combination of strong isolation, familiar tooling, and hierarchical
                organization makes workspaces offer an attractive solution to many of the problems
                commonly faced in multi-tenant environments: each tenant gets the freedom of a full
                dedicated cluster, yet operators manage only a single shared \gls{kcp} control
                plane.
            
            \subsubsection{API}\label{subsubsec:api}
                As previously noted, most \gls{k8s@gls} based multi-tenant systems currently require
                manual intervention by an administrator to deploy new tenants or modify resource
                allocation.
                However \gls{kcp}, other than similar frameworks, like Capsule or Kiosk, provides an
                \gls{api} server to the customer, that provides an easy way to access
                their resources \parencites[p.~651]{nguyen2022}{kcpio}.
                This in turn enables a degree of automation \parencite[p.~651]{nguyen2022}.
                Every workspace has its own \gls{api} endpoint \parencite{kcpWorkspaces}.
                This ensures, that more control can be shifted back to the customer.
                \gls{kcp} ships a curated set of built-in \gls{k8s@gls} \glspl{api}, such as
                \texttt{Namespaces}, \texttt{ConfigMaps}, \texttt{Secrets} and \gls{rbac} objects, 
                so tenants can start working with familiar primitives immediately
                \parencite{kcpAPIsBuiltIn}.
                Additional functionality can be added per workspace simply by installing a
                \gls{crd}, and \gls{kcp} permits multiple independent versions of the same \gls{crd}
                to coexist across workspaces \parencite{kcpAPIs}.
                \\
                To share an \gls{api} with other tenants, a provider declares an
                \texttt{APIResourceSchema} and then exports it through an \texttt{APIExport},
                while consumers attach that \gls{api} to their workspace with an \texttt{APIBinding}
                \parencite{kcpAPIsExport}.
                This export/bind workflow lets platform teams evolve \glspl{api} centrally without
                touching each consumer workspace, reinforcing the system's self-service goal
                \parencite{kcpAPIsExport}.
                \\
                \gls{kcp} supports \glspl{admissionWebhook} only through \gls{url}-based client
                configurations, while \texttt{service}-based webhooks and conversion webhooks are
                currently unsupported, so operators must host their hooks externally
                \parencite{kcpAPIsAdmissionWebhooks}.
                \\
                Because admission requests include the logical-cluster name, webhook back ends can
                enforce policies per workspace and thus maintain strong tenant isolation
                \parencite{kcpAuthorization}.
                Every \gls{rest} call is scoped under the path pattern
                \texttt{/clusters/<workspace>}, ensuring that automation never leaks objects across
                workspaces \parencite{kcpAPIsREST}.
                \gls{api} providers can also access consumer data through a virtual-workspace
                \gls{url} rooted at \texttt{/services/apiexport/…}, enabling safe cross-workspace
                reconciliation loops without cluster-wide privileges \parencite{kcpAPIsREST}.
                Together, built-in \glspl{api}, \glspl{crd}, the \gls{api}Export / \gls{api}Binding
                model, admission controls and workspace-prefixed \gls{rest} routing give tenants a
                rich yet safe surface for automation while keeping operational responsibility with
                the platform team.

            \subsubsection{Sharding}\label{subsubsec:sharding}
                \gls{kcp} employs sharding to \textbf{horizontally scale} the control-plane, letting
                an installation grow far beyond the limits of a single \gls{api}-server/\gls{etcd}
                pair \parencite{kcpSharding,kcpShardingShards}.
                \\
                Each shard hosts a set of logical clusters, so every workspace (and therefore every
                tenant) gets its own \gls{k8s@gls}-compatible consistency domain on that shard
                \parencite{kcpShardingShards}.
                Because \enquote{a set of known shards comprises a \gls{kcp} installation},
                operators can add or remove shards at will, expanding or contracting capacity with
                no downtime for existing workspaces \parencite{kcpShardingShards}.
                \\
                Cross-shard traffic is funneled through a dedicated \textbf{cache-server}, avoiding
                the \( n \times (n - 1) \) explosion of of direct links that would otherwise appear
                in large deployments \parencite{kcpShardingCacheServer}.
                This cache-server also underpins \textbf{workspace migration and object
                replication}, so tenants remain oblivious to topology changes while the platform
                evolves underneath them \parencite{kcpShardingCacheServer}.
                \\
                Administrators can define \textbf{Partitions} that group shards, for example by
                region or load profile, giving schedulers a topology-aware \gls{api} for controller
                placement \parencite{kcpShardingPartitions}.
                Partitions therefore deliver \textbf{geo-proximity, load distribution, and fault
                isolation} for multi-tenant control-plane components
                \parencite{kcpShardingPartitions}.
                Taken together, sharding provides the scalability, noisy-neighbor isolation, and
                topology flexibility required to run \textbf{large numbers of independent workspaces
                in a single multi-tenant \gls{kcp} deployment}.

            \subsubsection{High Availability}\label{subsubsec:highAvailability}
                As defined in~\cite[p.~3-3]{nist800-113} \gls{ha} \enquote{is a failover feature to
                ensure availability during device or component interruptions}.
                \\
                \gls{kcp} employs several mechanisms to ensure \gls{ha}.
                Firstly to achieve this “failover feature” at the control-plane level, \gls{kcp}
                relies heavily on its \textbf{cache-server layer}
                \parencite{kcpShardingCacheServer}.
                While individual shards can (and will) go offline, the cache server provides a
                logically-central, eventually-consistent replica of the small but critical objects
                that every shard must be able to see in order to keep tenant workspaces, \gls{api}
                bindings or scheduling decisions functioning \parencite{kcpShardingCacheServer}.
                In essence, the cache server acts as a rendez-vous point that collapses the
                \( n \times (n - 1) \) mesh of direct shard-to-shard links into a single, well-known
                endpoint \parencite{kcpShardingCacheServer}.
                Instead of every shard having to maintain and re-establish dozens of peer
                connections after a failure, each shard needs only a single healthy path to any
                replica of the cache tier \parencite{kcpShardingCacheServer}.
                This single-hop topology is easier to debug, cheaper to secure, and—most
                importantly—continues to deliver the global metadata that controllers require even
                when one or several shards are offline \parencite{kcpShardingCacheServer}.
                \\
                As described in~\cite{kcpShardingCacheServer}, \gls{kcp} uses a write-once /
                read-many replication model to populate and refresh that shared state:

                \begin{enumerate}[label={[\arabic*]:},
                    ref=Challenge~\arabic*,
                    leftmargin=*,
                    itemsep=0.6\baselineskip]

                    \item\label{chal:writeControllers}
                        \textit{Write controllers}
                        that run on every shard stream a selected set of objects, such as
                        \texttt{APIExport}, \texttt{APIResourceSchema}, \texttt{Shard}, certain
                        \gls{rbac} rules and more into the cache server.
                        Because the controller keeps its own authoritative copy, it can re-push data
                        after a transient outage without the risk of split-brain.
                        As noted by~\cite{redhatHA2021}, \gls{splitBrain} describes \enquote{a
                        phenomenon where the cluster is separated from communication but each part
                        continues working as separate clusters, potentially writing to the same data
                        and possibly causing corruption or loss.}.

                    \item\label{chal:readControllers}
                        \textit{Read controllers}
                        on all shards maintain informers that watch other shards' objects from the
                        cache server.
                        These informers are isolated from a shard's local \gls{etcd} and can
                        therefore start with a more tolerant back-off strategy: a shard may declare
                        itself \enquote{ready} for tenant traffic even if cache connectivity has not
                        yet been re-established, and will self-heal once the link returns.
                \end{enumerate}

                Because objects in the cache server are stored per logical cluster but can be listed
                via wildcard paths, each controller needs only one LIST/WATCH stream per resource
                type rather than per cluster \parencite{kcpShardingCacheServer}.
                This dramatically reduces the number of long-lived \gls{tcp} connections that must
                survive a fail-over and further increases control-plane availability.
                \\
                Consequently, no individual shard becomes a single point of failure for global
                control-plane metadata.
                Only the cache tier must be deployed in a highly available configuration ---
                something that can be achieved with standard Kubernetes Service + LoadBalancer
                constructs or by running two or more replicas behind a global \gls{ip} address.
                By funnelling cross-shard traffic through this purpose-built cache layer, \gls{kcp}
                delivers the fail-over semantics described by~\cite{nist800-113}: metadata remains
                available, and therefore the control plane remains operational, even when individual
                components fail.

        \newpage

        \subsection{SaaS Architecture and Automation}\label{subsec:saas}
            \gls{saas} is, above all else, a \textbf{business and software-delivery model} in which
            a provider offers its solution through a low-friction, service-centric model that
            maximizes value for customers and providers surrounding all tenant environments with a
            single, unified experience
            \parencites[pp.~3--4]{awsSaaSArchitectureFundamentals}[p.~11]{awsSaaSArchitectureFundamentals}.
            According to~\cite[pp.~3-4]{awsSaaSArchitectureFundamentals}, \gls{saas} is associated
            with six major objectives:

            \begin{enumerate}[label={[\arabic*]:},
                    ref=Challenge~\arabic*,
                    leftmargin=*,
                    itemsep=0.6\baselineskip]

                    \item\label{chal:saasAgility}
                        \textit{Agility}.
                        \gls{saas} companies prosper by designing for continuous adaptation to
                        evolving markets, customer demands, competitive pressures, pricing models,
                        and target segments.

                    \item\label{chal:saasOperationalEfficiency}
                        \textit{Operational efficiency}.
                        \gls{saas} companies grow and scale by fostering a culture of
                        \textbf{operational efficiency} that unifies tooling, enables rapid
                        collective deployment across all customer environments, and eliminates
                        one-off customizations.

                    \item\label{chal:saasFrictionlessOnboarding}
                        \textit{Frictionless onboarding}.
                        \gls{saas} providers must minimize friction in onboarding for every
                        \gls{b2b} and \gls{b2c} tenant by creating repeatable, efficient processes
                        that accelerate time-to-value.

                    \item\label{chal:saasInnovation}
                        \textit{Innovation}.
                        \gls{saas} providers build a flexible foundation that lets them respond to
                        current customer needs while using that same agility to innovate as well as
                        unlock new markets, opportunities, and efficiencies.

                    \item\label{chal:saasMarketResponse}
                        \textit{Market response}.
                        \gls{saas} replaces long-cycle releases with near-real-time agility,
                        enabling organizations to pivot strategy in response to emerging market
                        dynamics.

                    \item\label{chal:saasGrowth}
                        \textit{Growth}.
                        \gls{saas} is a growth-oriented model that promotes agility and efficiency,
                        enabling rapid adoption.

            \end{enumerate}

            Automation is the foundation for the utilization of the scaling effects that come along
            with \gls{saas} architectures.
            The onboarding service automatically orchestrates other services to
            create users, tenant, isolation policies, provision, and per-tenant resources
            \parencite[p.~14]{awsSaaSArchitectureFundamentals}.
            Once live, automated pipelines let new features roll to every tenant through a single,
            shared process, giving operators a single pane of glass for the whole estate
            \parencite[p.~10]{awsSaaSArchitectureFundamentals}.
            However merely automating the provisioning of each customer environment and offloading
            its management to an \gls{msp} still leaves tenants running potentially different,
            separately-operated versions \parencite[pp.~23--24]{awsSaaSArchitectureFundamentals}.
            Furthermore it distances the software provider from unified onboarding, operations, and
            customer insight—so automation alone creates an \gls{msp} setup, whereas true \gls{saas}
            requires one shared version and a single, provider-owned control plane for every tenant
            \parencite[pp.~23--24]{awsSaaSArchitectureFundamentals}.
            \\
            Ultimately, \gls{saas} depends on automated, repeatable workflows that remove internal
            and external friction, and ensure stability, efficiency and repeatability for this
            process \parencite[p.~14]{awsSaaSArchitectureFundamentals}.

    \section{State of the Art and Related Work}\label{sec:related}

        \subsection{Zero-Downtime Deployment Strategies}\label{subsec:zeroDowntime}
            % syncer / controller !
            % eventual consistency !

        \subsection{Kubernetes Scaling Methods}\label{subsec:k8sScaling}
            % HPA
            % other strats?

        \subsection{Multi-Tenancy Concepts in the Cloud}\label{subsec:mtConcepts}
            % Kiosk, Capsule, also AWS, Azure

        \subsection{KCP}\label{subsec:relatedKCP}
            % nguyen2022

    \section{Conceptual Design}\label{sec:concept}

        \subsection{Proposed Scenario}\label{subsec:scenario}
            To demonstrate the conception, implementation, and evaluation of a highly-scalable, 
            highly available SaaS platform on \gls{kcp}, the thesis adopts a deliberately
            light-weight yet realistic \textbf{case study}:
            \\
            A \textbf{\gls{smb} Web-Presence-as-a-Service}, whose sole purpose is to give
            \glspl{smb} an instantly available, single-page web presence.
            The service sits between \gls{diy} website builders and full custom agency work.
            Tenants enter their company facts, choose a theme, and receive a live, secure page,
            without touching infrastructure.
            A \gls{dataContract} is used to allow the frontend to be data-driven.
            
            \begin{table}[H]\label{tab:scenario}
                \centering{}
                \renewcommand{\arraystretch}{1.5}
                \begin{tabular}{lP{4cm}P{8cm}}
                    \toprule
                    Actor & Responsibility & Interaction Pattern \\
                    \midrule
                    \gls{smb} tenant & Enters or edits company information; chooses a theme & Write-heavy only at onboarding, then sporadic \\
                    End-User Visitor & Loads the generated page & Read-only; bursty traffic driven by marketing and search indexing \\
                    Platform Operator & Maintains themes, monitors capacity, rolls out new platform versions & \gls{devops} \\
                    \bottomrule
                \end{tabular}
                \caption{Actors, their responsibilities, and interaction patterns in the case case study}
            \end{table}
            
            This scenario is attractive for evaluating \gls{kcp} because it combines high
            \gls{multiTenancy} (potentially tens on thousands of logically isolated sites) with
            skewed workload characteristics, like very high read \gls{fanOut} combined with low
            per-tenant write rate.
            The workload stresses horizontal scalability, especially of the control plane, without
            the confounding complexity of rich business logic or deep data lineage.
            Moreover, the fact model is sufficiently small to allow \textit{stateless} page
            rendering, enabling the data plane to be scaled independently of the rendering tier.
            The following subsection translates this narrative into concrete requirements.
            

        \subsection{System Requirements}\label{subsec:requirements}

            \subsubsection{Functional Requirements}\label{subsubsec:fr}
                \begin{table}[H]\label{tab:fr}
                    \centering{}
                    \renewcommand{\arraystretch}{1.5}
                    \begin{tabular}{lcp{4cm}P{8cm}}
                        \toprule
                        F ID & Scope & Title & Description \\
                        \midrule
                        \hypertarget{f1}{F-01} & \textit{Tenant} & Company Profile \gls{crud} & A tenant owner shall be able to perform CRUD operations on the company profile (name, address, contact, logo, about-text) through the dashboard \gls{api}. \\
                        \hypertarget{f2}{F-02} & \textit{Tenant} & Service Catalog \gls{crud} & A tenant owner shall be able to manage a list of services/products. \\
                        \hypertarget{f3}{F-03} & \textit{Tenant} & Ratings \gls{crud} & Public visitors shall be able to post 1-to-5-star ratings with comments; the public \gls{api} shall list ratings. \\
                        \hypertarget{f4}{F-04} & \textit{Tenant} & Public Page Delivery & The public Next.js frontend shall render the latest template and tenant data via the per-tenant \gls{api}. \\
                        \hypertarget{f5}{F-05} & \textit{Platform} & Tenant Provisioning & The dashboard shall create a new \gls{kcp} workspace, deploy the tenant stack, allocate the \gls{sql} schema and return a public \gls{url} in~$\leq$~60~s. \\
                        \hypertarget{f6}{F-06} & \textit{Platform} & Config Storage and Retrieval & Template configuration for every tenant shall be stored under \texttt{/<tenant-id>/config.json} in the shared object bucket and fetched by the per-tenant \gls{api} at start-up or cache miss. \\
                        \hypertarget{f7}{F-07} & \textit{Tenant} & Config Caching & The per-tenant \gls{api} shall cache configuration in-memory with a \gls{ttl}~$\geq$~24~h, falling back to the bucket on cache miss or version bump. \\
                        \bottomrule
                    \end{tabular}
                    \caption{Functional Requirements}
                \end{table}

            \subsubsection{Non Functional Requirements}\label{subsubsec:nfr}

                % TODO: make this a longtable
                \begin{table}[H]\label{tab:nfr}
                    \centering{}
                    \renewcommand{\arraystretch}{1.5}
                    \begin{tabular}{lcp{4cm}P{8cm}}
                        \toprule
                        NF ID & Scope & Title & Description \\
                        \midrule
                        \hypertarget{nf1}{NF-01} & Tenant & Public Availability & The public page shall achieve $\geq$~99.95~\% monthly uptime; workspace moves or \gls{api} pod restarts shall cause 0 failed GET requests. \\
                        \hypertarget{nf2}{NF-02} & Platform & Dashboard Availability & The owner dashboard shall achieve $\geq$~99.5~\% monthly uptime (so brief maintenance windows are acceptable). \\
                        \hypertarget{nf3}{NF-03} & Tenant & Page Performance & p95 full-page load time shall be $\leq$~600~ms at 200~req/s from one region. \\
                        \hypertarget{nf4}{NF-04} & Tenant & \gls{api} Latency & p95 latency for GET \texttt{/api/config} and GET \texttt{/api/reviews} shall be $\leq$~150~ms under the same load. \\
                        \hypertarget{nf5}{NF-05} & Platform & Horizontal Scalability & The control plane shall handle at least 2 000 tenant workspaces with etcd IO $\leq$~70~\% and <~100~ms workspace-list latency. \\
                        \hypertarget{nf6}{NF-06} & Platform & Cache Consistency & Config cache shall reflect bucket changes within 5~min after a tenant triggers “publish” in the dashboard. \\
                        \hypertarget{nf7}{NF-07} & Platform & Tenant Isolation and Security & Data and traffic from one tenant shall not be accessible to another (\gls{rbac}, row-level security, network policies). \\
                        \hypertarget{nf8}{NF-08} & Platform & Data Durability & Reviews and config data shall have an \gls{rpo}~=~0~h (\gls{wal}-based backups) and an \gls{rto}~$\leq$~2~h via cross-region restore. \\
                        \hypertarget{nf9}{NF-09} & Platform & Disaster Recovery for Bucket & The shared bucket shall replicate to a secondary region with automatic fail-over; cached assets must remain available during a primary-region outage. \\
                        \hypertarget{nf10}{NF-10} & Operator & Maintainability / \gls{ci}/\gls{cd} & A full platform deployment (dashboard + initializer + tenant chart) shall complete via GitOps pipeline in~$\leq$~15~min. \\
                        \bottomrule
                    \end{tabular}
                    \caption{Non Functional Requirements}
                \end{table}

        \subsection{Architecture Design with KCP for SaaS}\label{subsec:architectureDesign}

        \subsection{Automated Deployment Strategies}\label{subsec:deploymentStrategies}

    \section{Prototypical Implementation}\label{sec:prototype}

        \subsection{Infrastructure with KCP}\label{subsec:infrastructure}

        \subsection[Tenant Provisioning]{Tenant Provisioning (Automation, Multi-Tenancy)}\label{subsec:tenantProvisioning}

        \subsection[Scaling Mechanisms]{Scaling Mechanisms (Horizontal Pod Autoscaler)}\label{subsec:scaling}

        \subsection[Monitoring and Logging]{Monitoring and Logging (Prometheus, Grafana)}\label{subsec:monitoring}

    \section{Evaluation}\label{sec:evaluation}

        \subsection[Performance Measurements]{Performance Measurements (Downtime, Latency, Scaling)}\label{subsec:performance}

        \subsection{Scaling Scenarios and Optimizations}\label{subsec:scalingOptimization}

        \subsection{Discussion of Results}\label{subsec:discussion}

    \section{Conclusion and Outlook}\label{sec:conclusion}

        \subsection{Summary}\label{subsec:summary}

        \subsection{Personal Conclusion}\label{subsec:personalConclusion}

        \subsection{Future Outlook}\label{subsec:outlook}

    \cleardoublepage{}
    \printbibliography[
        title = {References},
        heading = bibintoc
    ]

    \cleardoublepage{}
    \appendix

\end{document}
