% TODO: disable draft mode
\documentclass[11pt, a4paper, oneside, listof=totoc]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman, english]{babel}
\usepackage[onehalfspacing]{setspace}
\usepackage{helvet}
\usepackage[a4paper, left=2.5cm, right=2.5cm]{geometry}
\usepackage[
    backend=biber,
    style=authoryear,
    citestyle=authoryear,
    sorting=nyt,
    sortcites=true,
    dashed=false,
    giveninits=false,
    uniquename=true,
    maxbibnames=999,
    url=true
]{biblatex}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{courier}
\usepackage{needspace}
\usepackage{trace}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage[automark]{scrlayer-scrpage}
\usepackage{microtype}
\usepackage{svg}
\usepackage{xurl}
\usepackage{enumitem}
\usepackage{csquotes}
\usepackage[acronym]{glossaries}

% show frames in draft
%\usepackage{showframe}

% set language to english
\selectlanguage{english}

% set font to sans-serif
\renewcommand{\familydefault}{\sfdefault}

\addbibresource{jabref-library.bib}

% custom vars
\newcommand{\thesistitle}{Conception, Implementation, and Evaluation of a Highly Scalable and Highly Available Kubernetes-Based SaaS Platform on Kubernetes Control Plane (KCP)}
\newcommand{\sic}{\textnormal{\textit{[sic]}}}
\newcommand{\see}[1]{(see~\autoref{#1}: \textit{\nameref{#1}})}

% URL formatting
\PassOptionsToPackage{hyphens}{url}

% metadata
\title{\thesistitle}
\author{David Linhardt}
\date{\today}

\pagestyle{scrheadings}
\cfoot{\thepage} % Page number in center of footer

% glossary
\makeglossaries{}
\input{glossary.tex}

\begin{document}

    \newgeometry{top=1cm, bottom=1cm, left=2.5cm, right=2.5cm}

    \begin{titlepage}
        \thispagestyle{empty}

        \hspace*{-1.5cm}
        \noindent
        \hfill
        % THI Logo
        \begin{minipage}{0.3\textwidth}
            \raggedleft\
            \hspace*{1cm}
            \includegraphics[width=1\textwidth]{images/thi_logo.pdf}
        \end{minipage}

        \vspace{1cm}

        \hrulefill

        \vspace{4cm}

        % header box
        \noindent
        \makebox[\textwidth][c]{
            \parbox{10cm}{
                \LARGE\textbf{Technische Hochschule Ingolstadt}\\
                \\
                \Large\textbf{Specialist area Computer Science}\\
                \Large\textbf{Bachelor's course Computer Science}\\
            }
        }

        % bachelors thesis
        \begin{center}
            \LARGE\textbf{Bachelor's thesis}
        \end{center}

        \vspace{1cm}

        % tabularx
        \begin{tabularx}{\textwidth}{@{}lX@{}}
            \textbf{Subject:} & \thesistitle\\[2cm]
            \textbf{Name and Surname:} & David Linhardt \\[0.5cm]
            \textbf{Matriculation number:} & 00122706\\[2cm]     
            \textbf{Issued on:} & 2025--04--09 \\[0.5cm]           
            \textbf{Submitted on:} & 2025--09--09 \\[2cm]           
            \textbf{First examiner:} & Prof.\ Dr.\ Bernd Hafenrichter \\[0.5cm]      
            \textbf{Second examiner:} & Prof.\ Dr.\ Ludwig Lausser \\
        \end{tabularx}

    \end{titlepage}

    \restoregeometry

    \pagenumbering{roman}

    \section*{Abstract}\label{abstract}
    \markboth{Abstract}{Abstract}

    \newpage

    % TOC
    \begingroup
        \microtypesetup{protrusion=false}
        \tableofcontents
    \endgroup

    \newpage

    \printglossaries{}\label{glossary}
    \markboth{Glossary}{Glossary}

    \newpage

    \pagenumbering{arabic}

    \section{Introduction}\label{sec:introduction}

        \subsection{Problem Statement and Motivation}\label{subsec:problem}

        \subsection{Objectives and Scope}\label{subsec:objectives}

        \subsection{Structure of the Thesis}\label{subsec:structure}

    \section{Fundamentals}

        \subsection{Kubernetes and Multi-Tenancy}\label{subsec:k8sAndMultiTenancy}

            \subsubsection{Kubernetes as the Foundation for Cloud-Native Applications}\label{subsubsec:foundationK8s}
                As the de facto standard for deploying and managing 
                \textit{cloud-native applications}, Kubernetes, commonly referred to as \gls{k8s}
                plays a pivotal role in modern cloud architecture \parencite[p.~7--8]{poulton2021}.
                Kubernetes works as an  orchestrator for \textit{containerized,
                cloud-native microservice} applications, meaning it can deploy apps and dynamically
                respond to changes \parencite[p.~3]{poulton2021}.
                It offers a platform for declarative configuration and automation for containerized
                workloads, enabling organizations to run distributed applications and services at
                scale \parencite{kubernetesOverview,redhatWhatIsKubernetes}.

            \subsubsection{The Importance of Multi-Tenancy in Modern SaaS Platforms}\label{subsubsec:mtImportance}
                Multi-tenancy plays a fundamental role in modern cloud computing. 
                By allowing multiple tenants to share the same infrastructure through
                virtualization, it significantly increases resource utilization, reduces operational
                costs, and enables essential features such as VM mobility and dynamic resource
                allocation \parencite[pp.~345--346]{aljahdali2014}. 
                These benefits are crucial for cloud providers, as they make the cloud business
                model economically viable and scalable. 
                In the context of modern \gls{saas} platforms, multi-tenancy goes even further by
                enabling unified management, frictionless onboarding, and simplified operational
                processes that allow providers to add new tenants without introducing incremental
                complexity or cost \parencite[pp.~9--11]{awsSaaSArchitectureFundamentals}.
                \\
                However, while multi-tenancy is indispensable for achieving efficiency, scalability,
                and cost-effectiveness, it simultaneously introduces complex security challenges,
                especially in shared environments where resource isolation is limited. 
                In particular, the potential for cross-tenant access and side-channel attacks makes
                security in multi-tenant environments a primary concern
                \parencite[pp.~345--346]{aljahdali2014}. 
                As such, understanding and addressing multi-tenancy from both operational and
                security perspectives is essential when designing and securing modern cloud-native
                platforms
                \parencites[pp.~9--11]{awsSaaSArchitectureFundamentals}[p.~4]{isoConcepts}.

            \subsubsection{The Challenges of Multi-Tenancy and the Need for Solutions}\label{subsubsec:challenges}
                Multi-tenancy introduces a spectrum of technical and security challenges that need
                to be addressed.

                \begin{enumerate}[label={[\arabic*]:},
                    ref=Challenge~\arabic*,
                    leftmargin=*,
                    itemsep=0.6\baselineskip]

                    \item\label{chal:remanence}
                        \textit{Residual-data exposure}.
                        Shared infrastructures may expose tenants to data leakage and hardware-layer
                        attacks. 
                        Because hardware resources are only virtually partitioned,
                        residual data left in reusable memory or storage blocks,
                        known as \textit{data remanence},
                        can be inadvertently leaked or deliberately harvested by co-resident tenants
                        \parencites[p.~586]{zissis2012}[pp.~344--345]{aljahdali2014}.

                    \item\label{chal:transparency}
                        \textit{Control and transparency}.
                        By design, \gls{saas} moves both data storage and security controls out of
                        the enterprise's boundary and into the provider's multi-tenant cloud,
                        depriving organizations of direct oversight and assurance and thereby
                        heightening concern over how their critical information is protected,
                        replicated and kept available \parencite[pp.~3--4]{subashini2011}.
                        To complicate matters further, the customer might have no way to evaluate
                        the \gls{saas} vendors security measures, meaning the pricing and feature
                        set will most likely determine which service is used in practice, often
                        disregarding security concerns
                        \parencites[p.~6]{everett2009}[p.~836]{khorshed2012}.

                    \item\label{chal:scheduling}
                        \textit{Scheduling}.
                        In multi-tenant architectures multiple tenants utilize the same hardware,
                        thus creating the need for fair scheduling to ensure cost-effectiveness
                        and performance \parencite[p.~32597]{simi2024}.
                        Achieving fair and efficient resource allocation in scheduling first
                        requires a quantitative assessment of the system's existing unfairness
                        \parencites[p.~7]{ebrahimi2012}[p.~14]{beltre2019}[pp.~2--3]{ghodsi2011}.
                        Various scheduling algorithms and policies can be employed in practice to
                        achieve fairness \parencites[pp.~14--16]{beltre2019}[p.~4]{ghodsi2011}.
                        To fully leverage the advantages of multi-tenant architectures, resources
                        must not only be shared fairly, but also efficiently, not hindering
                        performance \parencite[p.~14]{beltre2019}.
                        As stated by~\cite[p.~14]{beltre2019} \enquote{Balancing both cluster
                        utilization and fairness is challenging}.
                    
                    \item\label{chal:isolation}
                        \textit{Performance Isolation}
                        A single tenant is able to significantly degrade the performance of other
                        tenants working on the same hardware, if \textit{performance isolation}
                        is not given \parencite[p.~195]{krebs2013}.
                        The fundamental performance expectations of a system are commonly formalized
                        in a \gls{sla}.
                        As noted by~\cite[p.~195]{krebs2013} \enquote{A system is said to be
                        performance isolated, if for tenants working within their quotas the
                        performance is within the (response time) \gls{sla} while other
                        tenants exceed their quotas (e.g., request rate)}.
                        As noted by~\cite[p.~18]{carrion2022} \enquote{Currently, it is difficult
                        to achieve performance isolation for multi-tenancy on \gls{k8s} clusters
                        because this requires providing resource isolation and improving the
                        abstraction of applications.}

                    \item\label{chal:automation}
                        \textit{Automation}.
                        As noted by~\cite[p.~651]{nguyen2022} \enquote{Presently, multi-tenant
                        systems lack the facility of allowing clients to dynamic~\sic{}
                        change their resources based on their business demands or create and
                        allocate resources for new tenants.
                        Multi-tenant system~\sic{} administrator manually does all the work of
                        allocation or changing tenant's~\sic{} resources.}
                        However, to ensure efficiency and scalability, an \gls{api} that allows
                        automating deployments and dynamic changes in the application is needed.

                \end{enumerate}

                A secure solution, keeping multi-tenancies advantages while also addressing security
                concerns is desperately needed
                \parencites[p.~346]{aljahdali2014}[pp.~14576--14577]{senel2023}.

            \subsubsection{Kubernetes Control Plane (KCP) as a Promising Approach}\label{subsubsec:kcpApproach}
                \gls{kcp} offers three capabilities that map cleanly onto today's multi-tenancy pain
                points.

                \begin{enumerate}[label={[\arabic*]:},
                    ref=Challenge~\arabic*,
                    leftmargin=*,
                    itemsep=0.6\baselineskip]

                    \item\label{chal:workspaces}
                        \textit{Workspaces}.
                        \gls{kcp} achieves strong resource isolation through the concept of
                        \textit{workspaces} \see{subsubsec:workspaces}.

                    \item\label{chal:api}
                        \textit{API}
                        \gls{kcp} offers an Kubernetes-like \gls{api} that enables the use of
                        standard tools and automation to a degree \see{subsubsec:api}.
                    
                    \item\label{chal:sharding}
                        \textit{Sharding}
                        \gls{kcp} offers sharding out of the box to manage high traffic and 
                        geo-distribution \see{subsubsec:sharding}.
                \end{enumerate}

            \subsubsection{Background: The Evolution of Kubernetes}\label{subsubsec:k8sEvolution}
                Kubernetes, an open-source container orchestration platform developed by Google,
                emerged from the need to manage the complexities of containerized applications
                effectively and to support large-scale deployments in a cloud-native environment
                \parencites{googlecloudWhatIsKubernetes}{kubernetesOverview}.
                It was originally developed at Google and released as open source in 2014
                \parencite{googlecloudWhatIsKubernetes}.
                Kubernetes was conceived as a successor to Google's internal container
                management system called Borg, and designed to streamline the process of deploying,
                scaling, and managing applications composed of microservices running in containers
                \parencites[pp.~13--14]{verma2015}[p.~84]{bernstein2014}.
                Since its inception, Kubernetes has gained traction among organizations because it
                provides robust features such as automated scaling,
                self-healing, and service discovery, which have made it the de facto standard for
                container orchestration in the tech industry
                \parencite[pp.~855--858]{damarapati2025}.
                \\
                As noted by~\cite[p.~457]{moravcik2022} by 2021 almost 90\% of organizations used
                Kubernetes as an orchestrator for managing containers and over 70\% of organizations
                used it in production \parencite{redhatStateOfK8sSecurityReport2021}.
                The widespread adoption of Kubernetes is further underscored by Red Hat's latest
                (2024) report, which no longer asks survey respondents if they use Kubernetes for
                container orchestration, but rather \textbf{which} Kubernetes platform they use
                \parencite[p.~27]{redhatStateOfK8sSecurityReport2024}.
                According to~\cite[pp.~855--856]{damarapati2025}, Kubernetes has seen
                unprecedented industry adoption due to its vendor neutrality, strong community
                support, and flexible, extensible architecture in combination with enterprise
                readiness caused by high availability, disaster recovery and security.
                \\
                Moreover Kubernetes enables faster time-to-market by providing a unified,
                declarative control plane that abstracts away infrastructure,
                guarantees consistent environments from development to production,
                and automates operational tasks such as scaling, rolling updates,
                and self-healing—advantages that translate directly into competitive delivery speed
                increasing its appeal to organizations of every size
                \parencite[pp.~858--859]{damarapati2025}.
                \\
                Over the years, Kubernetes—and the many orchestration solutions inspired by or built
                on it—has evolved to handle an increasingly diverse range of workloads,
                supporting everything from conventional applications to emerging
                \textit{edge-native} deployments \parencites[p.~21]{biot2025}[pp.~1--4]{biot2025}.
                Edge-native deployments are applications intended to run on computing
                resources located at or near the data source --- the network \textit{edge} ---
                rather than in a central cloud \parencite[p.~34]{satyanarayanan2019}.
                This adaptability reflects its fundamental design, which focuses on modularity and
                extensibility, allowing developers to customize their orchestration needs.
                \\
                Overall, the history of Kubernetes showcases a transformative journey driven by the
                evolving demands of software architecture and the necessity for efficient
                application management in an increasingly complex technological landscape.

            \subsubsection{Background: Containerization as an Enabler of Kubernetes}\label{subsubsec:containerization}
                \textit{Containerization} is a way to bundle an application's code with all its
                dependencies to run on any infrastructure thus enhancing portability
                \parencite{awsWhatIsContainerization,dockerWhatContainer}.
                The lightweight nature and isolation can be leveraged by cloud-native software by
                enabling vertical and horizontal autoscaling facilitated by quick container boot
                times, along with self-healing mechanisms and support for distributed, resilient
                infrastructures \parencites{kubernetesAutoscalingWorkloads}{kubernetesSelfHealing}
                    {awsWhatIsContainerization}[pp.~58--59]{davis2019}
                Furthermore it complements the microservice architectural pattern by enabling
                isolated, low overhead deployments, ensuring consistent environments
                \parencite[p.~209]{balalaie2016}.

            \subsubsection{Background: The Role of Microservices in Cloud-Native Architectures}\label{subsubsec:microservices}
                \textit{Microservices} play a pivotal role in cloud-native architectures by
                promoting agility, scalability, and maintainability of applications.
                By decomposing applications into independent, granular services, microservices
                facilitate development, testing, and deployment using diverse technology stacks,
                enhancing interoperability across platforms
                \parencites[p.~1]{waseem2020}[p.~1]{larrucea2018} and help prevent failures
                in one component from propagating across the system, by isolating functionality into
                distinct, self-contained services \parencite[p.~62]{davis2019}.
                This architectural style aligns well with cloud environments, as it allows services
                to evolve independently, effectively addressing challenges associated with scaling
                and maintenance without being tied to a singular technological framework
                \parencite[pp.~202--203]{balalaie2016}.
                Furthermore, the integration of microservices with platforms like Kubernetes
                enhances deployment automation and orchestration, thus providing substantial
                elasticity to accommodate fluctuating workloads \parencite[p.~170]{haugeland2021}.
                Additionally, migrating legacy applications to microservices can foster
                modernization and efficiency, thus positioning organizations favorably in
                competitive landscapes \parencite[p.~214]{balalaie2016}.
                Overall, the synergy between microservices and cloud-native architectures stems from
                their inherent capability to optimize resource utilization and streamline continuous
                integration and deployment processes.

            \subsubsection{Background: Kubernetes Resource Isolation Mechanisms}\label{subsubsec:k8sResourceIsolation}
                Kubernetes employs several resource isolation mechanisms, primarily through the use
                of \textit{cgroups} (control groups) and \textit{namespaces} to limit resource
                allocation for containers.
                Cgroups are a Linux kernel feature that organizes processes into hierarchical
                groups for fine-grained resource limitation and monitoring via a pseudo-filesystem
                called \textit{cgroupfs} \parencites{kubernetesCgroupsV2}{cgroups7}.
                \textit{Namespaces} are a mechanism for isolating groups of resources withing a
                single cluster and scoping resource names to prevent naming conflicts across
                different teams or projects \parencite{kubernetesNamespaces}.
                However, these mechanisms may not always provide sufficient isolation necessary for
                multi-tenant architectures, because the logical segregation offered by namespaces
                does not address the fundamental security concerns associated with multi-tenancy 
                \parencite[p.~651]{nguyen2022} and research indicates that the native isolation
                strategies can lead to performance interference, where containers that share nodes
                can experience significant degradation in performance due to CPU contention
                \parencite[p.~158]{kim2021}.
                Specifically, critical services may be adversely affected when non-critical services
                monopolize available resources, which undermines the quality of service in
                multi-tenant environments \parencite[p.~30410]{li2019}.
                \\
                Moreover, while Kubernetes allows for container orchestration and resource
                scheduling, it can lead to resource fragmentation, further exacerbating the issue
                of performance isolation \parencite[p.~1]{jian2023}.
                A common approach in multi-tenant scenarios is to deploy separate clusters for each
                tenant, which incurs substantial overhead—particularly in environments utilizing
                virtual machines for isolation \parencite[pp.~144574--144575]{senel2023}.
                In summary, although Kubernetes offers essential isolation mechanisms, the
                complexities of resource sharing and performance consistency in multi-tenant
                applications highlight the need for enhanced strategies to ensure robust resource
                management and performance isolation
                \parencites[p.~651]{nguyen2022}[p.~2]{jian2023}[p.~158]{kim2021}.

            \subsubsection{Relevance to SaaS and this Thesis}

        \newpage

        \subsection{Kubernetes Control Plane (KCP)}\label{subsec:kcp}
            \gls{kcp} is \enquote{An open source horizontally scalable control plane for
            Kubernetes-like APIs} \parencite{kcpio}.
            % nguyen 2022
            % KCP docs

            \subsubsection{Workspaces}\label{subsubsec:workspaces}
                \gls{kcp} introduces the concept of \textit{workspaces} to implement multi-tenancy.
                In \gls{kcp}, a workspace is a Kubernetes-cluster-like HTTPS endpoint exposed under
                \texttt{/clusters/<parent>:<name>}, that regular tools such as \textit{kubectl},
                \textit{Helm} or \textit{client-go} treat exactly like a real Kubernetes cluster.
                Every workspace is backed by its own logical cluster stored in an isolated
                \textbf{etcd prefix}, so objects in one workspace (including cluster-scoped
                resources like \textbf{CRDs}) are completely invisible to others, delivering hard
                multi-tenancy without spinning up separate control planes \parencite{kcpWorkspaces}.
                \\
                As per the definition by the~\cite{etcd}, \enquote{\textbf{etcd} is a strongly
                consistent, distributed key-value store that provides a reliable way to store data
                that needs to be accessed by a distributed system or cluster of machines.
                It gracefully handles leader elections during network partitions and can tolerate
                machine failure, even in the leader node.}
                etcd is the primary datastore used by \gls{kcp} and \gls{k8s}
                \parencites{kcpDevStorageToRest}[p.~214]{sun2021}.
                \textbf{etcd prefixes} are a simple, inbuilt way to group keys using a prefix
                \parencite{etcdPrefix}.
                This allows for resource isolation in \gls{kcp}.\@
                A \gls{crd} is a declaratively specified schema that registers a new  resource,
                defined by its group, version, kind, and OpenAPI schema, into the Kubernetes control
                plane so the native \gls{api} server stores and serves the objects as first-class
                resources \parencite{kubernetesCRD}.
                \\
                \gls{kcp} implements the same \gls{rbac}-based authorization mechanism and cluster
                role and cluster role binding principles as Kubernetes inside workspaces 
                \parencite{kcpAuthorization}.
                However unlike Kubernetes \gls{kcp} does currently not support \gls{abac}
                \parencites{kcpAuthorization}{kubernetesABAC}.
                \gls{kcp} likely supports only \gls{rbac}-based authorization because \gls{abac} is
                considered overly complex, hard to audit, and increasingly deprecated in favor of
                \gls{rbac}, which offers a more structured and maintainable access control model
                \parencite{kubernetesRBACBlog}.
                This allows for consistent access control and permission management across all
                workspaces, aligning with familiar Kubernetes patterns and simplifying multi-tenant
                environment administration.
                \\
                Workspaces for a typed, parent-child tree, and each type can constrain which kind
                of workspaces it can contain or be contained by, giving platform teams a structured
                way to delegate environments while retaining policy control
                \parencite{kcpWorkspaces}.
                \\
                This combination of strong isolation, familiar tooling, and hierarchical
                organization makes workspaces offer an attractive solution to many of the problems
                commonly faced in multi-tenant environments: each tenant gets the freedom of a full
                dedicated cluster, yet operators manage only a single shared \gls{kcp} control
                plane.
            
            \subsubsection{API}\label{subsubsec:api}
                As previously noted, most Kubernetes based multi-tenant systems currently require
                manual intervention by an administrator to deploy new tenants or modify resource
                allocation.
                However \gls{kcp}, other than similar frameworks, like Capsule or Kiosk, provides an
                \gls{api} server to the customer, that provides an easy way to access
                their resources \parencites[p.~651]{nguyen2022}{kcpio}.
                This in turn enables a degree of automation \parencite[p.~651]{nguyen2022}.
                Every workspace has its own \gls{api} endpoint \parencite{kcpWorkspaces}.
                This ensures, that more control can be shifted back to the customer.
                \gls{kcp} ships a curated set of built-in Kubernetes \glspl{api}, such as
                \texttt{Namespaces}, \texttt{ConfigMaps}, \texttt{Secrets} and \gls{rbac} objects, 
                so tenants can start working with familiar primitives immediately
                \parencite{kcpAPIsBuiltIn}.
                Additional functionality can be added per workspace simply by installing a
                \gls{crd}, and \gls{kcp} permits multiple independent versions of the same \gls{crd}
                to coexist across workspaces \parencite{kcpAPIs}.
                \\
                To share an \gls{api} with other tenants, a provider declares an
                \texttt{APIResourceSchema} and then exports it through an \texttt{APIExport},
                while consumers attach that \gls{api} to their workspace with an \texttt{APIBinding}
                \parencite{kcpAPIsExport}.
                This export/bind workflow lets platform teams evolve \glspl{api} centrally without
                touching each consumer workspace, reinforcing the system's self-service goal
                \parencite{kcpAPIsExport}.
                \\
                \gls{kcp} supports admission webhooks only through \gls{url}-based client
                configurations, \texttt{service}-based webhooks and conversion webhooks are
                currently unsupported, so operators must host their hooks externally
                \parencite{kcpAPIsAdmissionWebhooks}.
                \\
                Because admission requests include the logical-cluster name, webhook back-ends can
                enforce policies per workspace and thus maintain strong tenant isolation
                \parencite{kcpAuthorization}.
                Every \gls{rest} call is scoped under the path pattern
                \texttt{/clusters/<workspace>}, ensuring that automation never leaks objects across
                workspaces \parencite{kcpAPIsREST}.
                \gls{api} providers can also access consumer data through a virtual-workspace
                \gls{url} rooted at \texttt{/services/apiexport/…}, enabling safe cross-workspace
                reconciliation loops without cluster-wide privileges \parencite{kcpAPIsREST}.
                Together, built-in \glspl{api}, \glspl{crd}, the \gls{api}Export / \gls{api}Binding
                model, admission controls and workspace-prefixed \gls{rest} routing give tenants a
                rich yet safe surface for automation while keeping operational responsibility with
                the platform team.

            \subsubsection{Sharding}\label{subsubsec:sharding}
                \gls{kcp} employs sharding to \textbf{horizontally scale} the control-plane, letting
                an installation grow far beyond the limits of a single \gls{api}-server/etcd
                pair \parencite{kcpSharding,kcpShardingShards}.
                \\
                Each shard hosts a set of logical clusters, so every workspace (and therefore every
                tenant) gets its own Kubernetes-compatible consistency domain on that shard
                \parencite{kcpShardingShards}.
                Because \enquote{a set of known shards comprises a \gls{kcp} installation},
                operators can add or remove shards at will, expanding or contracting capacity with
                no downtime for existing workspaces \parencite{kcpShardingShards}.
                \\
                Cross-shard traffic is funneled through a dedicated \textbf{cache-server}, avoiding
                the \( n \times (n - 1) \) explosion of of direct links that would otherwise appear
                in large deployments \parencite{kcpShardingCacheServer}.
                This cache-server also underpins \textbf{workspace migration and object
                replication}, so tenants remain oblivious to topology changes while the platform
                evolves underneath them \parencite{kcpShardingCacheServer}.
                \\
                Administrators can define \textbf{Partitions} that group shards, for example by
                region or load profile, giving schedulers a topology-aware \gls{api} for controller
                placement \parencite{kcpShardingPartitions}.
                Partitions therefore deliver \textbf{geo-proximity, load distribution, and fault
                isolation} for multi-tenant control-plane components
                \parencite{kcpShardingPartitions}.
                Taken together, sharding provides the scalability, noisy-neighbor isolation, and
                topology flexibility required to run \textbf{large numbers of independent workspaces
                in a single multi-tenant \gls{kcp} deployment}.
        \newpage

        \subsection{SaaS Architecture and Automation}\label{subsec:saas}
            \gls{saas} is, above all else, a \textbf{business and software-delivery model} in which
            a provider offers its solution through a low-friction, service-centric model that
            maximizes value for customers and providers surrounding all tenant environments with a
            single, unified experience
            \parencites[pp.~3--4]{awsSaaSArchitectureFundamentals}[p.~11]{awsSaaSArchitectureFundamentals}.
            According to~\cite[pp.~3-4]{awsSaaSArchitectureFundamentals}, \gls{saas} is associated
            with six major objectives:

            \begin{enumerate}[label={[\arabic*]:},
                    ref=Challenge~\arabic*,
                    leftmargin=*,
                    itemsep=0.6\baselineskip]

                    \item\label{chal:saasAgility}
                        \textit{Agility}.
                        \gls{saas} companies prosper by designing for continuous adaptation to
                        evolving markets, customer demands, competitive pressures, pricing models,
                        and target segments.

                    \item\label{chal:saasOperationalEfficiency}
                        \textit{Operational efficiency}
                        \gls{saas} companies grow and scale by fostering a culture of
                        \textbf{operational efficiency} that unifies tooling, enables rapid
                        collective deployment across all customer environments, and eliminates
                        one-off customizations.

                    \item\label{chal:saasFrictionlessOnboarding}
                        \textit{Frictionless onboarding}
                        \gls{saas} providers must minimize friction in onboarding every \gls{b2b}
                        and \gls{b2c} tenant by creating repeatable, efficient processes that
                        accelerate time-to-value.

                    \item\label{chal:saasInnovation}
                        \textit{Innovation}
                        \gls{saas} providers build a flexible foundation that lets them respond to
                        current customer needs while using that same agility to innovate and unlock
                        new markets, opportunities, and efficiencies.

                    \item\label{chal:saasMarketResponse}
                        \textit{Market response}
                        \gls{saas} replaces long-cycle releases with near-real-time agility,
                        enabling organizations to pivot strategy in response to emerging market
                        dynamics.

                    \item\label{chal:saasGrowth}
                        \textit{Growth}
                        \gls{saas} is a growth-oriented model that aligns organizational agility and
                        efficiency to welcome rapid adoption of the offering. 

            \end{enumerate}

            Automation is the foundation for utilizing the scaling effects that come with \gls{saas}
            architectures. The onboarding service automatically orchestrates other services to
            create users, tenant, isolation policies, provision and per-tenant resources
            \parencite[p.~14]{awsSaaSArchitectureFundamentals}.
            Once live, automated pipelines let new features roll to every tenant through a single,
            shared process, giving operators a single pane of glass for the whole estate
            \parencite[p.~10]{awsSaaSArchitectureFundamentals}.
            However merely automating the provisioning of each customer environment and offloading
            its management to an \gls{msp} still leaves tenants running potentially different,
            separately-operated versions and distances the software provider from unified
            onboarding, operations, and customer insight—so automation alone creates an \gls{msp}
            setup, whereas true \gls{saas} requires one shared version and a single, provider-owned
            control plane for every tenant \parencite[pp.~23--24]{awsSaaSArchitectureFundamentals}.
            \\
            Ultimately, \gls{saas} depends on automated, repeatable workflows that remove internal
            and external friction and ensure stability, efficiency and repeatability for this
            process \parencite[p.~14]{awsSaaSArchitectureFundamentals}. 

    \section{State of the Art and Related Work}\label{sec:related}

        \subsection{Zero-Downtime Deployment Strategies}\label{subsec:zeroDowntime}
            % syncer / controller !
            % eventual consistency !

        \subsection{Kubernetes Scaling Methods}\label{subsec:k8sScaling}

        \subsection{Multi-Tenancy Concepts in the Cloud}\label{subsec:mtConcepts}

        \subsection{KCP}\label{subsec:relatedKCP}
            % nguyen2022

    \section{Conceptual Design}\label{sec:concept}

        \subsection{System Requirements}\label{subsec:requirements}

        \subsection{Architecture Design with KCP for SaaS}\label{subsec:architectureDesign}

        \subsection{Automated Deployment Strategies}\label{subsec:deploymentStrategies}

    \section{Prototypical Implementation}\label{sec:prototype}

        \subsection{Infrastructure with KCP}\label{subsec:infrastructure}

        \subsection[Tenant Provisioning]{Tenant Provisioning (Automation, Multi-Tenancy)}\label{subsec:tenantProvisioning}

        \subsection[Scaling Mechanisms]{Scaling Mechanisms (Horizontal Pod Autoscaler)}\label{subsec:scaling}

        \subsection[Monitoring and Logging]{Monitoring and Logging (Prometheus, Grafana)}\label{subsec:monitoring}

    \section{Evaluation}\label{sec:evaluation}

        \subsection[Performance Measurements]{Performance Measurements (Downtime, Latency, Scaling)}\label{subsec:performance}

        \subsection{Scaling Scenarios and Optimizations}\label{subsec:scalingOptimization}

        \subsection{Discussion of Results}\label{subsec:discussion}

    \section{Conclusion and Outlook}\label{sec:conclusion}

        \subsection{Summary}\label{subsec:summary}

        \subsection{Personal Conclusion}\label{subsec:personalConclusion}

        \subsection{Future Outlook}\label{subsec:outlook}

    \cleardoublepage{}
    \printbibliography[
        title = {References},
        heading = bibintoc
    ]

    \cleardoublepage{}
    \listoffigures

    \cleardoublepage{}

    \appendix

\end{document}
